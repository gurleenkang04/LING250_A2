{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a42fb913-439a-4ca3-a2e8-6b69428a4b70",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023c7935-c624-4f7b-aab3-ccb57bb71421",
   "metadata": {},
   "source": [
    "By Gurleen, Kyleigh, Yolanda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164de1eb-dd68-4b34-987e-0dec06b7998d",
   "metadata": {},
   "source": [
    "Used DigitalOcean tutorial to implement sentiment analysis.\n",
    "https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk#splitting-the-dataset-for-training-and-testing-the-model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97c8843-2d00-4ca7-a6d7-3b8f10941655",
   "metadata": {},
   "source": [
    "### Extract file with review, run this once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8b89beb8-fb3a-43f0-a69f-5bd05c600803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gurle\\AppData\\Local\\Temp\\ipykernel_9844\\869521061.py:7: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(path='dataset')\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "filename = 'dataset/review_polarity.tar.gz'\n",
    "\n",
    "# open and extract\n",
    "with tarfile.open(filename, 'r:gz') as tar:\n",
    "    tar.extractall(path='dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7563b1c-674b-4908-b3b7-682e7698470d",
   "metadata": {},
   "source": [
    "### Installing NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7fda7ebd-e575-4781-b8bf-4ceec563b922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gurle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gurle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\gurle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gurle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re, string\n",
    "import random\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2b9f86-cd23-4a86-af35-87c908f71381",
   "metadata": {},
   "source": [
    "### Downloading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4767023d-9df9-4877-b829-0d0f46c86e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_reviews(pos_folder, neg_folder):\n",
    "    texts, labels = [], []\n",
    "\n",
    "    # Load positive reviews\n",
    "    for file in os.listdir(pos_folder):\n",
    "        if file.endswith('.txt'):\n",
    "            with open(os.path.join(pos_folder, file), 'r', encoding='utf-8') as f:\n",
    "                texts.append(f.read())\n",
    "                labels.append(1)  # Positive label\n",
    "\n",
    "    # Load negative reviews\n",
    "    for file in os.listdir(neg_folder):\n",
    "        if file.endswith('.txt'):\n",
    "            with open(os.path.join(neg_folder, file), 'r', encoding='utf-8') as f:\n",
    "                texts.append(f.read())\n",
    "                labels.append(0)  # Negative label\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "pos_folder_path = 'dataset/txt_sentoken/pos' \n",
    "neg_folder_path = 'dataset/txt_sentoken/neg' \n",
    "\n",
    "texts, labels = load_reviews(pos_folder_path, neg_folder_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0dafe3-7f79-48dd-8224-f042b3c8d676",
   "metadata": {},
   "source": [
    "### Tokenizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "71596644-e7ff-4177-92f0-3cd79daaa047",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = [word_tokenize(text.lower()) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c45055-795d-4154-b4ca-484fd1672c0a",
   "metadata": {},
   "source": [
    "### Normalizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d377bcee-918c-4d11-84f8-1efc3de972e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming (lemmatization)\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "             pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f620ac-0355-46b7-b397-620f3ad4eda3",
   "metadata": {},
   "source": [
    "### Removing Noise from the Data (using regular expressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "677b6c54-8084-4eef-92f2-84ca7b7f1d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['film', 'adapt', 'comic', 'book', 'plenty', 'success', 'whether', \"'re\", 'superheroes', 'batman', 'superman', 'spawn', 'gear', 'toward', 'kid', 'casper', 'arthouse', 'crowd', 'ghost', 'world', \"'s\", 'never', 'really', 'comic', 'book', 'like', 'hell', 'starter', 'create', 'alan', 'moore', 'eddie', 'campbell', 'bring', 'medium', 'whole', 'new', 'level', 'mid', \"'80s\", '12-part', 'series', 'call', 'watchman', 'say', 'moore', 'campbell', 'thoroughly', 'research', 'subject', 'jack', 'ripper', 'would', 'like', 'say', 'michael', 'jackson', 'start', 'look', 'little', 'odd', 'book', '``', 'graphic', 'novel', '``', '500', 'page', 'long', 'include', 'nearly', '30', 'consist', 'nothing', 'footnote', 'word', \"n't\", 'dismiss', 'film', 'source', 'get', 'past', 'whole', 'comic', 'book', 'thing', 'might', 'find', 'another', 'stumble', 'block', 'hell', \"'s\", 'director', 'albert', 'allen', 'hughes', 'get', 'hughes', 'brother', 'direct', 'seem', 'almost', 'ludicrous', 'cast', 'carrot', 'top', 'well', 'anything', 'riddle', 'better', 'direct', 'film', \"'s\", 'set', 'ghetto', 'feature', 'really', 'violent', 'street', 'crime', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', 'ghetto', 'question', 'course', 'whitechapel', '1888', 'london', \"'s\", 'east', 'end', \"'s\", 'filthy', 'sooty', 'place', 'whore', 'call', '``', 'unfortunates', '``', 'start', 'get', 'little', 'nervous', 'mysterious', 'psychopath', 'carve', 'profession', 'surgical', 'precision', 'first', 'stiff', 'turn', 'copper', 'peter', 'godley', 'robbie', 'coltrane', 'world', 'enough', 'call', 'inspector', 'frederick', 'abberline', 'johnny', 'depp', 'blow', 'crack', 'case', 'abberline', 'widower', 'prophetic', 'dream', 'unsuccessfully', 'try', 'quell', 'copious', 'amount', 'absinthe', 'opium', 'upon', 'arrive', 'whitechapel', 'befriend', 'unfortunate', 'name', 'mary', 'kelly', 'heather', 'graham', 'say', \"n't\", 'proceeds', 'investigate', 'horribly', 'gruesome', 'crime', 'even', 'police', 'surgeon', 'ca', \"n't\", 'stomach', \"n't\", 'think', 'anyone', 'need', 'brief', 'jack', 'ripper', 'wo', \"n't\", 'go', 'particular', 'say', 'moore', 'campbell', 'unique', 'interesting', 'theory', 'identity', 'killer', 'reason', 'choose', 'slay', 'comic', \"n't\", 'bother', 'cloak', 'identity', 'ripper', 'screenwriter', 'terry', 'hayes', 'vertical', 'limit', 'rafael', 'yglesias', 'les', 'mis', 'rables', 'good', 'job', 'keep', 'hide', 'viewer', 'end', \"'s\", 'funny', 'watch', 'local', 'blindly', 'point', 'finger', 'blame', 'jew', 'indian', 'englishman', 'could', 'never', 'capable', 'commit', 'ghastly', 'act', 'hell', \"'s\", 'end', 'whistle', 'stonecutter', 'song', 'simpson', 'day', '``', 'hold', 'back', 'electric', 'car/who', 'make', 'steve', 'guttenberg', 'star', '``', \"n't\", 'worry', \"'ll\", 'make', 'sense', 'see', 'onto', 'hell', \"'s\", 'appearance', \"'s\", 'certainly', 'dark', 'bleak', 'enough', \"'s\", 'surprising', 'see', 'much', 'look', 'like', 'tim', 'burton', 'film', 'planet', 'ape', 'time', 'seem', 'like', 'sleepy', 'hollow', '2', 'print', 'saw', \"n't\", 'completely', 'finish', 'color', 'music', 'finalize', 'comment', 'marilyn', 'manson', 'cinematographer', 'peter', 'deming', \"n't\", 'say', 'word', 'ably', 'capture', 'dreariness', 'victorian-era', 'london', 'help', 'make', 'flashy', 'killing', 'scene', 'remind', 'crazy', 'flashback', 'twin', 'peak', 'even', 'though', 'violence', 'film', 'pale', 'comparison', 'black-and-white', 'comic', 'oscar', 'winner', 'martin', 'child', 'shakespeare', 'love', 'production', 'design', 'turn', 'original', 'prague', 'surroundings', 'one', 'creepy', 'place', 'even', 'acting', 'hell', 'solid', 'dreamy', 'depp', 'turn', 'typically', 'strong', 'performance', 'deftly', 'handle', 'british', 'accent', 'ians', 'holm', 'joe', 'gould', \"'s\", 'secret', 'richardson', '102', 'dalmatian', 'log', 'great', 'support', 'role', 'big', 'surprise', 'graham', 'cringe', 'first', 'time', 'open', 'mouth', 'imagine', 'attempt', 'irish', 'accent', 'actually', \"n't\", 'half', 'bad', 'film', 'however', 'good', '2', '00', 'r', 'strong', 'violence/gore', 'sexuality', 'language', 'drug', 'content'], ['every', 'movie', 'come', 'along', 'suspect', 'studio', 'every', 'indication', 'stinker', 'everybody', \"'s\", 'surprise', 'perhaps', 'even', 'studio', 'film', 'become', 'critical', 'darling', 'mtv', 'film', '_election', 'high', 'school', 'comedy', 'star', 'matthew', 'broderick', 'reese', 'witherspoon', 'current', 'example', 'anybody', 'know', 'film', 'exist', 'week', 'open', 'plot', 'deceptively', 'simple', 'george', 'washington', 'carver', 'high', 'school', 'student', 'election', 'tracy', 'flick', 'reese', 'witherspoon', 'over-achiever', 'hand', 'raise', 'nearly', 'every', 'question', 'way', 'way', 'high', 'mr', '``', '``', 'matthew', 'broderick', 'sick', 'megalomaniac', 'student', 'encourage', 'paul', 'popular-but-slow', 'jock', 'run', 'paul', \"'s\", 'nihilistic', 'sister', 'jump', 'race', 'well', 'personal', 'reason', 'dark', 'side', 'sleeper', 'success', 'expectation', 'low', 'go', 'fact', 'quality', 'stuff', 'make', 'review', 'even', 'enthusiastic', 'right', 'ca', \"n't\", 'help', 'go', 'baggage', 'glow', 'review', 'contrast', 'negative', 'baggage', 'reviewer', 'likely', '_election', 'good', 'film', 'live', 'hype', 'make', '_election_', 'disappointing', 'contain', 'significant', 'plot', 'detail', 'lift', 'directly', '_rushmore_', 'release', 'month', 'early', 'similarity', 'staggering', 'tracy', 'flick', '_election_', 'president', 'extraordinary', 'number', 'club', 'involve', 'school', 'play', 'max', 'fischer', '_rushmore_', 'president', 'extraordinary', 'number', 'club', 'involve', 'school', 'play', 'significant', 'tension', '_election_', 'potential', 'relationship', 'teacher', 'student', 'significant', 'tension', '_rushmore_', 'potential', 'relationship', 'teacher', 'student', 'tracy', 'flick', 'single', 'parent', 'home', 'contribute', 'drive', 'max', 'fischer', 'single', 'parent', 'home', 'contribute', 'drive', 'male', 'bumble', 'adult', '_election_', 'matthew', 'broderick', 'pursue', 'extramarital', 'affair', 'get', 'catch', 'whole', 'life', 'ruin', 'even', 'get', 'bee', 'sting', 'male', 'bumble', 'adult', '_rushmore_', 'bill', 'murray', 'pursue', 'extramarital', 'affair', 'get', 'catch', 'whole', 'life', 'ruin', 'get', 'several', 'bee', 'sting', 'happen', 'individual', 'screenplay', '_rushmore_', 'novel', '_election_', 'contain', 'many', 'significant', 'plot', 'point', 'yet', 'film', 'probably', 'even', 'aware', 'make', 'two', 'different', 'studio', 'genre', 'high', 'school', 'geek', 'revenge', 'movie', \"n't\", 'fully', 'form', 'yet', 'even', 'strength', '_election_', 'rely', 'upon', 'fantastic', 'performance', 'broderick', 'witherspoon', 'newcomer', 'jessica', 'campbell', 'paul', \"'s\", 'anti-social', 'sister', 'tammy', 'broderick', 'play', 'mr', 'rooney', 'role', '_ferris', 'bueller_', 'seem', 'fun', \"'s\", 'since', 'witherspoon', 'revelation', \"'s\", 'early', 'year', \"'s\", 'comedy', 'teenager', 'little', 'clout', 'money', 'witherspoon', 'deserves', 'oscar', 'nomination', 'campbell', \"'s\", 'character', 'get', 'go', 'like', 'fantastic', 'speech', 'gymnasium', \"'re\", 'win', 'one', 'thing', \"'s\", 'bother', 'since', \"'ve\", 'see', 'extraordinary', 'amount', 'sexuality', 'film', 'suppose', 'come', 'mtv', 'film', 'expect', 'less', 'film', 'start', 'light', 'airy', 'like', 'sitcom', 'screw', 'tighten', 'tension', 'mount', 'alexander', 'payne', 'decides', 'add', 'element', 'frankly', 'distract', 'story', 'bad', 'enough', 'mr', \"n't\", 'like', 'tracy', \"'s\", 'determination', 'win', 'cost', 'throw', 'student/teacher', 'relationship', 'even', \"'s\", 'logical', 'reason', 'mr', 'affair', \"'s\", 'lot', 'like', '_election_', 'plot', 'similarity', '_rushmore_', 'tonal', 'nosedive', 'take', 'get', 'explicitly', 'sex-driven', 'mark', 'disappointment'], [\"'ve\", 'get', 'mail', 'work', 'alot', 'good', 'deserve', 'order', 'make', 'film', 'success', 'cast', 'two', 'extremely', 'popular', 'attractive', 'star', 'share', 'screen', 'two', 'hour', 'collect', 'profit', 'real', 'acting', 'involve', 'original', 'inventive', 'bone', \"'s\", 'body', \"'s\", 'basically', 'complete', 're-shoot', 'shop', 'around', 'corner', 'add', 'modern', 'twist', 'essentially', 'go', 'defies', 'concept', 'good', 'contemporary', 'filmmaking', \"'s\", 'overly', 'sentimental', 'time', 'terribly', 'mushy', 'mention', 'manipulative', 'oh', 'enjoyable', 'manipulation', 'must', 'something', 'casting', 'manipulation', 'make', 'movie', 'work', 'well', 'absolutely', 'hat', 'previous', 'ryan/hanks', 'team', 'sleepless', 'seattle', 'could', \"n't\", 'directing', 'film', 'helm', 'woman', \"n't\", 'quite', 'yet', 'figure', 'like', 'much', \"'ve\", 'get', 'mail', 'really', 'important', 'like', 'something', 'much', 'even', 'question', 'storyline', 'cliched', 'come', 'tom', 'hank', 'play', 'joe', 'fox', 'insanely', 'likeable', 'owner', 'discount', 'book', 'chain', 'meg', 'ryan', 'play', 'kathleen', 'kelley', 'even', 'insanely', 'likeable', 'proprietor', 'family-run', 'child', \"'s\", 'book', 'shop', 'call', 'nice', 'homage', 'shop', 'around', 'corner', 'fox', 'kelley', 'soon', 'become', 'bitter', 'rival', 'new', 'fox', 'book', 'store', 'open', 'right', 'across', 'block', 'small', 'business', 'little', 'know', 'already', 'love', 'internet', 'neither', 'party', 'know', 'person', \"'s\", 'true', 'identity', 'rest', 'story', \"n't\", 'important', 'serve', 'mere', 'backdrop', 'two', 'star', 'share', 'screen', 'sure', 'mildly', 'interesting', 'subplots', 'fail', 'comparison', 'utter', 'cuteness', 'main', 'relationship', 'course', 'lead', 'predictable', 'climax', 'foreseeable', 'end', \"'s\", 'damn', 'cute', 'well-done', 'doubt', 'movie', 'entire', 'year', 'contain', 'scene', 'evokes', 'much', 'pure', 'joy', 'part', 'ryan', 'discover', 'true', 'identity', 'online', 'love', 'fill', 'lack', 'good', 'word', 'happiness', 'first', 'time', 'year', 'actually', 'leave', 'theater', 'smiling']]\n"
     ]
    }
   ],
   "source": [
    "def remove_noise(tokens, stop_words = ()):\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        # Remove URLs\n",
    "        token = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', token)\n",
    "        # Remove mentions\n",
    "        token = re.sub(r\"(@[A-Za-z0-9_]+)\", \"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "\n",
    "    return cleaned_tokens\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# clean the tokenized movie reviews\n",
    "cleaned_tokens_list = [remove_noise(tokens, stop_words) for tokens in text_tokens]\n",
    "\n",
    "# print a few cleaned tokens to verify\n",
    "print(cleaned_tokens_list[:3]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10554711-c846-4bff-9c6f-e65b0a99d1fd",
   "metadata": {},
   "source": [
    "### Preparing Data for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "44fdb7de-542d-4261-9262-bcb52c68c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertings tokens to a dictionary\n",
    "\n",
    "def get_reviews_for_model(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tokens)\n",
    "\n",
    "review_tokens_for_model = get_reviews_for_model(cleaned_tokens_list)\n",
    "\n",
    "# splitting the dataset for training and testing the model\n",
    "dataset = [(review_dict, \"Positive\" if label == 1 else \"Negative\")\n",
    "           for review_dict, label in zip(review_tokens_for_model, labels)]\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "split_index = int(0.8 * len(dataset)) # 80% for training, 20% for testing\n",
    "train_data = dataset[:split_index]\n",
    "test_data = dataset[split_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fbe9bd-7233-4825-a5b1-90ed98d454b6",
   "metadata": {},
   "source": [
    "### Building and Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "253fad48-ada9-4da1-a87e-ac94b482f7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.7575\n",
      "Most Informative Features\n",
      "             outstanding = True           Positi : Negati =     14.2 : 1.0\n",
      "               ludicrous = True           Negati : Positi =     13.0 : 1.0\n",
      "                    plod = True           Negati : Positi =     11.6 : 1.0\n",
      "                 b-movie = True           Negati : Positi =     11.0 : 1.0\n",
      "                   poker = True           Positi : Negati =     10.4 : 1.0\n",
      "              degenerate = True           Negati : Positi =     10.3 : 1.0\n",
      "               stupidity = True           Negati : Positi =     10.2 : 1.0\n",
      "                  turkey = True           Negati : Positi =      9.8 : 1.0\n",
      "            exploitation = True           Negati : Positi =      9.6 : 1.0\n",
      "               insulting = True           Negati : Positi =      9.6 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e36ff34-876d-4ccb-913f-347c33f916f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
